{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Do: Crude Stopwords\n",
    "For this activity, create a function that takes in an article and outputs a list of words that is free of stopwords and any non-letter characters. After looking at the results, define your own list of stopwords to add to the NLTK default set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\arjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Code to download corpora\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store an article to work with\n",
    "crude_article = reuters.raw(fileids=reuters.fileids(categories='crude')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TURKEY CALLS FOR DIALOGUE TO SOLVE DISPUTE\n",
      "  Turkey said today its disputes with\n",
      "  Greece, including rights on the continental shelf in the Aegean\n",
      "  Sea, should be solved through negotiations.\n",
      "      A Foreign Ministry statement said the latest crisis between\n",
      "  the two NATO members stemmed from the continental shelf dispute\n",
      "  and an agreement on this issue would effect the security,\n",
      "  economy and other rights of both countries.\n",
      "      \"As the issue is basicly political, a solution can only be\n",
      "  found by bilateral negotiations,\" the statement said. Greece has\n",
      "  repeatedly said the issue was legal and could be solved at the\n",
      "  International Court of Justice.\n",
      "      The two countries approached armed confrontation last month\n",
      "  after Greece announced it planned oil exploration work in the\n",
      "  Aegean and Turkey said it would also search for oil.\n",
      "      A face-off was averted when Turkey confined its research to\n",
      "  territorrial waters. \"The latest crises created an historic\n",
      "  opportunity to solve the disputes between the two countries,\"\n",
      "  the Foreign Ministry statement said.\n",
      "      Turkey's ambassador in Athens, Nazmi Akiman, was due to\n",
      "  meet Prime Minister Andreas Papandreou today for the Greek\n",
      "  reply to a message sent last week by Turkish Prime Minister\n",
      "  Turgut Ozal. The contents of the message were not disclosed.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the stored article\n",
    "print(crude_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the `clean_text` function\n",
    "def clean_text(article):\n",
    "    \n",
    "    # Define a set of stopwords using `stopwords.words()`\n",
    "\n",
    "    # Define the regex parameters\n",
    "\n",
    "    # Apply regex parameters to article\n",
    "\n",
    "    # Apply `word_tokenize` to the regex scrubbed text\n",
    "\n",
    "    # Create list of lower-case words that are not in the stopword set\n",
    "    \n",
    "    # Return the final list\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass article into `clean_text` and store the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique words using the `set` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second iteration, with custom stopwords\n",
    "def clean_text(article):\n",
    "    \n",
    "    # Define a set of stopwords using `stopwords.words()`\n",
    "    \n",
    "    # Create custom stopwords\n",
    "\n",
    "    # Define the regex parameters\n",
    "\n",
    "    # Apply regex parameters to article\n",
    "\n",
    "    # Apply `word_tokenize` to the regex scrubbed text\n",
    "\n",
    "    # Create list of lower-case words that are not in the stopword set\n",
    "    \n",
    "    # Return the final list\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass article into `clean_text` and examine new results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
